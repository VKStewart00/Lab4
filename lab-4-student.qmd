---
title: 'Lab 4'
author: 'Van K Stewart'
ouput: html_document
embed-resources: true
code-tools: true
echo: true
---

# Part 1: Setup

## GitHub Workflow

Set up your GitHub workflow (either using the method of creating a repository and using Version Control to set up your project or vice versa using the `usethis` package commands we have learned).

Use appropriate naming conventions for your project (see Code Style Guide), e.g. `lab-4-import-data`.  

Your project folder should contain the following:  

- .Rproj  
- `lab-4-student.qmd` 
- "data-raw" folder
    + `buoy_2024.txt`
- "data-clean" folder that contains final data set  
- rendered document  
- rendered document (`.md`)  

## Seeking Help

Part of learning to program is learning from a variety of resources. Thus, I expect you will use resources that you find on the internet. There is, however, an important balance between copying someone else's code and *using their code to learn*. Therefore, if you use external resources, I want to know about it.

-   If you used Google, you are expected to "inform" me of any resources you used by **pasting the link to the resource in a code comment next to where you used that resource**.

-   If you used ChatGPT, you are expected to "inform" me of the assistance you received by (1) indicating somewhere in the problem that you used ChatGPT (e.g., below the question prompt or as a code comment), and (2) downloading and including the `.txt` file containing your **entire** conversation with ChatGPT in your repository. ChatGPT can we used as a "search engine", but you should not copy and paste prompts from the lab or the code into your lab.

Additionally, you are permitted and encouraged to work with your peers as you complete lab assignments, but **you are expected to do your own work**. Copying from each other is cheating, and letting people copy from you is also cheating. Please don't do either of those things.

## Lab Instructions

The questions in this lab are noted with numbers and boldface. Each question will require you to produce code, whether it is one line or multiple lines.

This document is quite plain, meaning it does not have any special formatting. As part of your demonstration of creating professional looking Quarto documents, **I would encourage you to spice your documents up (e.g., declaring execution options, specifying how your figures should be output, formatting your code output, etc.).**

## Setup

In the code chunk below, load in the packages necessary for your analysis. You should only need the `tidyverse` package for this analysis.

```{r}
#| label: setup

library(tidyverse)
library(janitor)

```

# Part 2: Data Context

Throughout the Monterey Bay there are various NOAA stations that capture data on wind speed, wave height, air temperature and many other data points. Data such as this can be used to predict weather or tidal event or used in conjunction with other data, such as population estimates of sardines or whale migration, to understand the ocean ecosystem.

::::: {layout="[ 70, 30 ]"}
::: {#first-column}
For this lab, we will focus on [Station 46092](https://www.ndbc.noaa.gov/station_history.php?station=46092), which is maintained by MBARI (Monterey Bay Aquarium Research Institute). which is on the outer edge of the Monterey Bay Canyon. Data is collected and stored hourly, but sometimes there are issues with censors not working or other events that lead to missing data.
:::

::: {#second-column}
![](images/mbari-station-46092.png){fig-alt="A topographical map of the Monterey Bay, CA region with the buoy 46092 indicated near the Monterey Bay Cayon outer edge."}
:::
:::::

We will be looking at a historical file from 2024 to examine air temperature trends over the last year. Historical files have gone through post-processing analysis and represent the data sent to the archive centers. The formats for both are generally the same, with the major difference being the treatment of missing data. Missing data in the Historical files are variable number of 9's, depending on the data type (for example: 999.0 99.0).

In addition, the data file also stores a "comment" in the second row that contains the units measured for each variable. Read more about the [variables stored in the historical files here](https://www.ndbc.noaa.gov/faq/measdes.shtml)

Finally, notice that the data storage type is `txt` which means we cannot use `read_csv()` to import the data. Instead

## Step 1: Reading the Data into `R`
Read the data into R. Since it is a `txt` file it is considered a "whitespace-separated file".  Use the (readr)[https://readr.tidyverse.org/index.html] website to determine the appropriate `read_XXX()` function.  

When you read in the data give it an appropriate name for what the data represents. The read in file should look like this (note this is just the first seven columns):  

```{r}
#| label: load-data

# Need to tell R the path to the data

buoyData <- read_table("data-raw/buoy_2024.txt"
                       )


```

Notice that the data is all read in as a character value and the first row contains information that is not data, but additional information about the variables: this is called a "comment" in the data.  

## Step 2: Specify the Comment Row  
In your data, you will notice the `#` in front of `yr` in the second row. That is the symbol that indicates a comment in the data set. Using the cheatsheet or the (readr)[https://readr.tidyverse.org/index.html] website, add an argument to your function that indicates that the comment row begins with `#`.  Your data should now look like this:

```{r}
#| label: load-data-comment

buoyData2 <- read_table("data-raw/buoy_2024.txt",
                        comment = "#")

```


## Step 3: Clean the Variable Names  
We want to use snake_case for our variable names. Use the `janitor` package to clean the variable names. Notice, that because month was originally coded `MM` and minute was originally coded `mm` that now month is `mm` and minute is `mm_2` to create a distinction between them.  

```{r}
#| label: clean-names

# set # as the comment
buoyData3 <- read_table("data-raw/buoy_2024.txt",
                        comment = "#"
                        )|>
                        clean_names()
                        

```

## Step 4: Write the Data Part 1
We will have a bit of cleaning to do on the data, but we have an issue. The 99 and 999's in the data are numeric and not character values. In order for us to specify them as `NA`s we need to do a little work around.  

Write the data to a `csv` file called `buoy_update.csv`. Be sure to write it to your `data-raw` folder.  

```{r}
#| label: write-data-part-1

#up to date buoy data, set '99' and '999' as N/A values 
buoyData3 <- read_table("data-raw/buoy_2024.txt",
                        comment = "#"
                        )|>
                        clean_names()

#write the formatted file into a csv
write_csv(buoyData3, "data-raw/buoy_update.csv")
```



## Step 5: Dealing with Missing Values  
Now we will read the data back in, be sure to assign the it a meaningful object name, **BUT** we are going to do two things:  

1. Specify all column type *defaults* as character.  
2. Specify `NA`s as "99" and "999"

Remember that now we are reading back in a data file of type `csv`. Now when you look at the data we should have `NA` instead of `99` or `999` in our data, for example:

```{r}
#| label: load-data-again

#loading data, but as a csv this time

buoyUpdate <- read_csv( "data-raw/buoy_update.csv",
                       na = c('99','999', 'NA'))
```

This is great! But we created another problem. Now all of our numeric values are being treated like characters and we don't want that. There are a couple of ways to fix this. We can write and reread in the data setting the column types to `double` or we can use the `readr` parsing functions. 

Here is the code you can use to fix the data without exporting and importing again (we will learn more about the `mutate()` function. Insert whatever you called your buoy data on the last import for .  

```{r}
#| label: parse-double

buoyUpdate <- buoyUpdate |> mutate(across(everything(), as.character))
buoy_final <- buoyUpdate |> 
  mutate(across(.cols = everything(), ~parse_double(.x)))
```



Now we have our clean, correctly classified, data ready for analysis as `buoy_final`. Go ahead and write that data as a `csv` file into your `data-clean` file. Once you've done that. Set the data cleaning code chunks to `eval: false` so that you do not rerun them again when you render the document. 


# Part 3: Recreate the Following Visualization
Now that are data is "analytic ready", go ahead and read in your clean data and use it to try and recreate the following visualization. Do your best to match the following visualization as close as possible. (Hint: when specifying color, you may need to use `factor()` to get R to treat a continuous value as discrete).  

```{r}
#| label: monterey-air-temp-viz


# I thought Id be using this more than once...
i <- aes(mm, 
         atmp)

# Stores the colors I like as an objest to be used later.  
j <- c("#FF83FA", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "#1FFFD4", "#EE3B3B", "#FF7F00", "#EE1289", "#1874CD", "#B03060" )

# Picks the dataset, and the year I want to look at
filter(buoy_final, 
       yy == 2024
       )|>

# Sets up a graphic for me
ggplot()+
   
# A smooth line to help track air temp average over 2024 
geom_smooth(i)+
  
  # Does the nice points
  geom_point(aes(x = mm,
         y = atmp,
         color = factor(mm)
         ))+
  
    # Uses the colors picked above
    scale_color_manual(values = j)+
  
    # Removes the legend
    theme(legend.position = "none")+
  
    # Lables my Axis
    labs( x = "Month in 2024",
          y = 'Air Temperature (Celsius)',
          title = 'Monterey Bay Caynon Air Temperature' )
  



```

# Part 4: Find a Data Set

For your final portfolio, you will choose a data set to clean and provide an analysis of through visualization (or more if you have the experience). For this week, choose a data set

-   It is a real-world application of R that has not exactly been worked out before (e.g. it isn't a demo from some package or blog).\
-   It is interesting to you.\
-   It involves data and analyzing or presenting that data. The data may be data you have from a lab, or something you have retrieved from the web, some examples of good sources: FBI database on crime statistics, National Oceanic and Atmospheric Administration, World Health Organization, Twitter, Yahoo finance data, etc. If you are having problems finding a dataset, see the resources at the end of the project description.\
-   The analysis and presentation is useful in the real-world.

One fun source for data is [Tidy Tuesday](https://github.com/rfordatascience/tidytuesday) which posts a new data set almost every week. It is a great place to find ideas and sources.

**For your found data write a brief description of the data and and provide its source.**

> Carbon Majors Emissions Data:  This dataset consist of carbon emissions spanning back to 1854.  Some possible questions; Which industry is responsiable for the most amount of CO2 emissions, or which level of company (State, private, or National) is responsible for the most emissions.  




# Lab 4 Submission

For Lab 4 you will submit the link to your GitHub repository. Your rendered file is required to have the following specifications in the YAML options (at the top of your document):

-   have the plots embedded (`embed-resources: true`)
-   include your source code (`code-tools: true`)
-   include all your code and output (`echo: true`)

**If any of the options are not included, your Lab 3 or Challenge 3 assignment will receive an "Incomplete" and you will be required to submit a revision.**

In addition, your document should not have any warnings or messages output in your HTML document. **If your HTML contains warnings or messages, you will receive an "Incomplete" for document formatting and you will be required to submit a revision.**